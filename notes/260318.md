## What I've learnt?

Multiple iterations in Adam optimizer in Keras may lead to numerical instability. Generally LSTM
doesn't save us from exploding gradient - it only gives us a somewhat decent learning rate because
of loopholing the error via the state mode. Generally, whole mess with neural networks is about 
providing reasonable differentials of the error function to every node in the network and turn
it to reasonable weight changes. Adam optimizer on default settings has neglibile learning rate 
after many iterations and this causes absurd weight updates. Changing the beta parameters from
(0.9, 0.999 to 0.999, 0.999999) fixes the problem.

The network effect in humans is kinda great, if you can estimate personality traits more accurately
by whom you talk to than what you say. [This article is an interesting read on this][1].

## What I want to know?

1. Updates should be equal to 1e-3 of inputs, but why?
2. What is deep belief network?
3. What is extreme learning machine?

## What I want to write about?

1. Finish implementation of Dense in C#
2. How to get DATA?
3. Comprehension is compression https://arxiv.org/pdf/1802.07044.pdf
4. Batch size modification https://arxiv.org/pdf/1711.00489.pdf


[1]: https://blog.acolyer.org/2017/02/16/beyond-the-words-predicting-user-personality-from-heterogeneous-information/
